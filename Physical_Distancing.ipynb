{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Physical Distance maintaing among the crowd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Learning<br>\n",
    "<h5>Deep learning is a type of machine learning and artificial intelligence (AI) that imitates the way humans gain certain types of knowledge.<br>\n",
    " Deep learning is an important element of data science, which includes statistics and predictive modelling. It is extremely beneficial to data scientists who are tasked with collecting, analysing and interpreting large amounts of data deep learning makes this process faster and easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Function for people detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, recPers=0):\n",
    "\t(H, W) = frame.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False) # scaling the frames to convert all the image\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#in the range of 0-255\n",
    "\tnet.setInput(blob)\n",
    "\tlayerOutputs = net.forward(ln) #provide nparray to draw bounding box\n",
    "\n",
    "\tboxes = []  #bounding boxes\n",
    "\tcentroids = [] #centeroids\n",
    "\tconfidences = [] #probability list for detecting a  class \n",
    "\n",
    "\tfor output in layerOutputs:\n",
    "\t\tfor detection in output:\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t#filtering persons from image and setting its probability\n",
    "\t\t\tif classID == recPers and confidence > 0.4:\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t#finding the corners\n",
    "\t\t\t\tx = int(centerX - (width / 2)) \n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.4, 0.2) #used for suppression overlapping bounding boxes\n",
    "\n",
    "\tif len(idxs) > 0:\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(r)\n",
    "\n",
    "\treturn results #returning the list consist of max person probability \n",
    "\t\t\t\t\t#and bounding box co- ordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"yolo-coco\"\n",
    "\n",
    "labelsPath = os.path.sep.join([MODEL_PATH, \"coco.names\"]) #list of names on which models were prepared\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "weightsPath = os.path.sep.join([MODEL_PATH, \"yolov4-tiny.weights\"]) \n",
    "configPath = os.path.sep.join([MODEL_PATH, \"yolov4-tiny.cfg\"])\n",
    "\n",
    "#loading module into the program\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = net.getLayerNames() #gives you a list of all layers used \n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()] #gives you the index of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] accessing video stream...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] accessing video stream...\")\n",
    "#cap = cv2.VideoCapture(\"pedestrians.mp4\")\n",
    "#cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.VideoCapture(\"vid_short.mp4\")\n",
    "cap = cv2.VideoCapture(\"input2.mp4\")\n",
    "writer = None\n",
    "\n",
    "while True:\n",
    "\t# read the next frame from the file\n",
    "\t(ret, frame) = cap.read()\n",
    "\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\n",
    "\tframe = imutils.resize(frame, width=900)\n",
    "\tresults = detect_people(frame, net, ln, recPers=LABELS.index(\"person\"))\n",
    "\n",
    "\tviolate = set() #it contains set of violation pair\n",
    "\n",
    "\tif len(results) >= 2:\n",
    "\t\tcentroids = np.array([r[2] for r in results])\n",
    "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\t\tfor i in range(0, D.shape[0]):\n",
    "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
    "\t\t\t\tif D[i, j] < 75.0:\n",
    "\t\t\t\t\tviolate.add(i)\n",
    "\t\t\t\t\tviolate.add(j)\n",
    "\n",
    "\t# We use BGR format for coloring, here we are coloring the bounding boxes\n",
    "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "\t\t(startX, startY, endX, endY) = bbox\n",
    "\t\tcolor = (0, 255, 0)\t\n",
    "\n",
    "\t\t#update color to red for violation\n",
    "\t\tif i in violate:\n",
    "\t\t\tcolor = (0, 0, 255)\n",
    "\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "\t#Output Frame\n",
    "\ttext = \"Vioations : {}\".format(len(violate))\n",
    "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 0, 5), 3)\n",
    "\n",
    "\t#Displaying Results\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tif cv2.waitKey(1) & 0xFF==ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "\tif  writer is None:\n",
    "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "\tif writer is not None:\n",
    "\t\twriter.write(frame)\n",
    "\n",
    "cap.release() #releasing the resource\n",
    "\n",
    "cv2.destroyAllWindows()\t"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e0b39fdc4f2df2992ef135d097972ec225e019845a073717c5b1bc8df292c8a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
